{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c43b9200-6145-476c-8b28-b0588dd63741",
   "metadata": {},
   "source": [
    "### 1.- Realizar todos los ejercicios propuestos de libro (excepto los de hive en caso de utilizar spark instalado en local y en el caso de RDBMS hacer únicamente ejemplo especificado más adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0aca1f-47d3-4b2d-8fe4-4abf23a822ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69892d4d-2e90-4564-b00d-ccea8fe8cbe1",
   "metadata": {},
   "source": [
    "### 2.- Pros y Cons utilizar UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8aa29b-3e86-4b63-bd29-681baef76074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5264187-ff49-4e4d-be4d-6688ad9c08a2",
   "metadata": {},
   "source": [
    "### 3.- Instalar MySQL, descargar driver y cargar datos de BBDD de empleados [Base de datos](https://dev.mysql.com/doc/employee/en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09a389-bd38-468e-94fc-24dc4e724738",
   "metadata": {},
   "source": [
    "#### Cargar con spark datos de empleados y departamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aeae9b2-23c1-45c8-9d31-dc36f138d879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees: org.apache.spark.sql.DataFrame = [emp_no: int, birth_date: date ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In Scala\n",
    "// Loading data from a JDBC source using load \n",
    "val employees = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"employees\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"apl31/5/98ALEX\")\n",
    " .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddefac75-6931-453a-b67f-c9303f856f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8717740-f62b-4883-89b2-d1a82d7fda04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "departments: org.apache.spark.sql.DataFrame = [dept_no: string, dept_name: string]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val departments = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"departments\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"apl31/5/98ALEX\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8ba614-1fc5-4c1c-acca-a71af4879b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_no|         dept_name|\n",
      "+-------+------------------+\n",
      "|   d009|  Customer Service|\n",
      "|   d005|       Development|\n",
      "|   d002|           Finance|\n",
      "|   d003|   Human Resources|\n",
      "|   d001|         Marketing|\n",
      "|   d004|        Production|\n",
      "|   d006|Quality Management|\n",
      "|   d008|          Research|\n",
      "|   d007|             Sales|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27641860-1eb5-4152-8219-a9af712ff659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dept_manager: org.apache.spark.sql.DataFrame = [emp_no: int, dept_no: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dept_manager = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"dept_manager\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"apl31/5/98ALEX\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bcfba26-5eb9-4660-a346-c36aed48e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "|110022|   d001|1985-01-01|1991-10-01|\n",
      "|110039|   d001|1991-10-01|9999-01-01|\n",
      "|110085|   d002|1985-01-01|1989-12-17|\n",
      "|110114|   d002|1989-12-17|9999-01-01|\n",
      "|110183|   d003|1985-01-01|1992-03-21|\n",
      "|110228|   d003|1992-03-21|9999-01-01|\n",
      "|110303|   d004|1985-01-01|1988-09-09|\n",
      "|110344|   d004|1988-09-09|1992-08-02|\n",
      "|110386|   d004|1992-08-02|1996-08-30|\n",
      "|110420|   d004|1996-08-30|9999-01-01|\n",
      "|110511|   d005|1985-01-01|1992-04-25|\n",
      "|110567|   d005|1992-04-25|9999-01-01|\n",
      "|110725|   d006|1985-01-01|1989-05-06|\n",
      "|110765|   d006|1989-05-06|1991-09-12|\n",
      "|110800|   d006|1991-09-12|1994-06-28|\n",
      "|110854|   d006|1994-06-28|9999-01-01|\n",
      "|111035|   d007|1985-01-01|1991-03-07|\n",
      "|111133|   d007|1991-03-07|9999-01-01|\n",
      "|111400|   d008|1985-01-01|1991-04-08|\n",
      "|111534|   d008|1991-04-08|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_manager.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f88825-1f69-43aa-8b4a-d74b28758f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dept_emp: org.apache.spark.sql.DataFrame = [emp_no: int, dept_no: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dept_emp = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"dept_emp\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"apl31/5/98ALEX\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117c1dc2-f9d6-4f10-8c8b-cac7b6d78ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "| 10003|   d004|1995-12-03|9999-01-01|\n",
      "| 10004|   d004|1986-12-01|9999-01-01|\n",
      "| 10005|   d003|1989-09-12|9999-01-01|\n",
      "| 10006|   d005|1990-08-05|9999-01-01|\n",
      "| 10007|   d008|1989-02-10|9999-01-01|\n",
      "| 10008|   d005|1998-03-11|2000-07-31|\n",
      "| 10009|   d006|1985-02-18|9999-01-01|\n",
      "| 10010|   d004|1996-11-24|2000-06-26|\n",
      "| 10010|   d006|2000-06-26|9999-01-01|\n",
      "| 10011|   d009|1990-01-22|1996-11-09|\n",
      "| 10012|   d005|1992-12-18|9999-01-01|\n",
      "| 10013|   d003|1985-10-20|9999-01-01|\n",
      "| 10014|   d005|1993-12-29|9999-01-01|\n",
      "| 10015|   d008|1992-09-19|1993-08-22|\n",
      "| 10016|   d007|1998-02-11|9999-01-01|\n",
      "| 10017|   d001|1993-08-03|9999-01-01|\n",
      "| 10018|   d004|1992-07-29|9999-01-01|\n",
      "| 10018|   d005|1987-04-03|1992-07-29|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564734d5-10e7-4932-9026-fa46042fbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val titles = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"titles\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"apl31/5/98ALEX\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e95947-c186-4e13-aea3-af72f877ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|\n",
      "| 10001| 80013|1996-06-23|1997-06-23|\n",
      "| 10001| 81025|1997-06-23|1998-06-23|\n",
      "| 10001| 81097|1998-06-23|1999-06-23|\n",
      "| 10001| 84917|1999-06-23|2000-06-22|\n",
      "| 10001| 85112|2000-06-22|2001-06-22|\n",
      "| 10001| 85097|2001-06-22|2002-06-22|\n",
      "| 10001| 88958|2002-06-22|9999-01-01|\n",
      "| 10002| 65828|1996-08-03|1997-08-03|\n",
      "| 10002| 65909|1997-08-03|1998-08-03|\n",
      "| 10002| 67534|1998-08-03|1999-08-03|\n",
      "+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c31f9151-e553-4e2f-92d7-a7a56a99d489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salaries: org.apache.spark.sql.DataFrame = [emp_no: int, salary: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val salaries = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"salaries\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"apl31/5/98ALEX\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ab715b-3db2-479d-9f1c-72a31e8c5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|\n",
      "| 10001| 80013|1996-06-23|1997-06-23|\n",
      "| 10001| 81025|1997-06-23|1998-06-23|\n",
      "| 10001| 81097|1998-06-23|1999-06-23|\n",
      "| 10001| 84917|1999-06-23|2000-06-22|\n",
      "| 10001| 85112|2000-06-22|2001-06-22|\n",
      "| 10001| 85097|2001-06-22|2002-06-22|\n",
      "| 10001| 88958|2002-06-22|9999-01-01|\n",
      "| 10002| 65828|1996-08-03|1997-08-03|\n",
      "| 10002| 65909|1997-08-03|1998-08-03|\n",
      "| 10002| 67534|1998-08-03|1999-08-03|\n",
      "+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4d3c6-8ad6-4559-8619-22110fa4f113",
   "metadata": {},
   "source": [
    "#### Mediante Joins mostrar toda la información de los empleados además de su título y salario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e78f2edf-c5c3-46dc-8982-cd9fbeccb2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultado: org.apache.spark.sql.DataFrame = [emp_no: int, birth_date: date ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultado = employees.as(\"a\").join(salaries.as(\"b\"), employees(\"emp_no\") === salaries(\"emp_no\"), \"inner\")\n",
    "                                .join(dept_emp.as(\"c\"), employees(\"emp_no\") === dept_emp(\"emp_no\"), \"inner\")\n",
    "                                .join(departments.as(\"d\"), dept_emp(\"dept_no\") === departments(\"dept_no\"), \"inner\")\n",
    "                                .select(\"a.emp_no\", \"a.birth_date\", \"a.first_name\",  \"a.last_name\", \"a.gender\", \"a.hire_date\",\"b.salary\",\"d.dept_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4856281c-f8e5-4b4b-b1b7-baecce012026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|salary|  dept_name|\n",
      "+------+----------+----------+---------+------+----------+------+-----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 40000|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 43519|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 46265|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 46865|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 47837|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 52042|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 52370|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 53202|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 56087|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 59252|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 62716|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 67137|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 67944|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 67588|Development|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 71052|Development|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 60268|Development|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 61073|Development|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 63219|Development|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 67084|Development|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 66979|Development|\n",
      "+------+----------+----------+---------+------+----------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87bcdec5-7a11-40ed-b90c-6e2aa41200fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " cannot resolve '`titles.title`' given input columns: [employees.birth_date, titles.emp_no, salaries.emp_no, employees.emp_no, employees.first_name, titles.from_date, salaries.from_date, employees.gender, employees.hire_date, employees.last_name, titles.salary, salaries.salary, titles.to_date, salaries.to_date]; line 1 pos 69;\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve '`titles.title`' given input columns: [employees.birth_date, titles.emp_no, salaries.emp_no, employees.emp_no, employees.first_name, titles.from_date, salaries.from_date, employees.gender, employees.hire_date, employees.last_name, titles.salary, salaries.salary, titles.to_date, salaries.to_date]; line 1 pos 69;\r",
      "'Project [birth_date#151, first_name#152, last_name#153, 'titles.title, salary#126]\r",
      "+- Join Inner, (emp_no#150 = emp_no#125)\r",
      "   :- Join Inner, (emp_no#150 = emp_no#57)\r",
      "   :  :- SubqueryAlias employees\r",
      "   :  :  +- Relation[emp_no#150,birth_date#151,first_name#152,last_name#153,gender#154,hire_date#155] JDBCRelation(employees) [numPartitions=1]\r",
      "   :  +- SubqueryAlias titles\r",
      "   :     +- Relation[emp_no#57,salary#58,from_date#59,to_date#60] JDBCRelation(salaries) [numPartitions=1]\r",
      "   +- SubqueryAlias salaries\r",
      "      +- Relation[emp_no#125,salary#126,from_date#127,to_date#128] JDBCRelation(salaries) [numPartitions=1]\r",
      "\r",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:158)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:155)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:342)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:342)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r",
      "  at scala.collection.immutable.List.map(List.scala:298)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:155)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:184)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:178)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:230)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:175)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r",
      "  ... 36 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "employees.createOrReplaceTempView(\"employees\")\n",
    "salaries.createOrReplaceTempView(\"salaries\")\n",
    "titles.createOrReplaceTempView(\"titles\")\n",
    "spark.sql(\"\"\"SELECT employees.birth_date,employees.first_name,employees.last_name,titles.title,salaries.salary\n",
    "FROM employees\n",
    "JOIN titles ON employees.emp_no= titles.emp_no\n",
    "JOIN salaries ON employees.emp_no= salaries.emp_no\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa69adc-ae89-482b-a53f-25a1539eff45",
   "metadata": {},
   "source": [
    "#### Diferencia entre Rank y dense_rank (operaciones de ventana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5526e81-964f-4b63-8868-6a52a3879229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
